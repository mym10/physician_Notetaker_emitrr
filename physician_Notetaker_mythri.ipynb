{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup & Libraries"
      ],
      "metadata": {
        "id": "QhXUKKNE0F9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g86yfVhu0cVN",
        "outputId": "d30bef50-ef73-47d8-9788-9b68506938d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.7.5)\n",
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.12/dist-packages (from scispacy) (6.0.0)\n",
            "Requirement already satisfied: nmslib-metabrainz==2.1.3 in /usr/local/lib/python3.12/dist-packages (from scispacy) (2.1.3)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.12/dist-packages (from scispacy) (0.3.4)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from nmslib-metabrainz==2.1.3->scispacy) (5.9.5)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from keybert) (5.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz (119.8 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from en_ner_bc5cdr_md==0.5.4) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2025.8.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_ner_bc5cdr_md==0.5.4) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn spacy scispacy keybert\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n",
        "\n",
        "#Imports\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
        "from keybert import KeyBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preprocessing"
      ],
      "metadata": {
        "id": "gxz6B7410Nd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_transcript(raw_transcript: str):\n",
        "    lines = [line.strip() for line in raw_transcript.split('\\n') if line.strip() != '']\n",
        "\n",
        "    dialogue = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"[\") and line.endswith(\"]\"):\n",
        "            dialogue.append({'speaker': 'Event', 'text': line.strip(\"[]\")})\n",
        "            continue\n",
        "\n",
        "        match = re.match(r\"^(Doctor|Dr\\.|Physician|Patient|Pt|Ms\\.?\\s*Jones?):\\s*(.*)$\", line, re.IGNORECASE)\n",
        "        if match:\n",
        "            speaker = match.group(1).capitalize()\n",
        "            text = match.group(2).strip()\n",
        "            dialogue.append({'speaker': speaker, 'text': text})\n",
        "    return dialogue\n",
        "\n",
        "def combine_dialogue(dialogue):\n",
        "    # Concatenate all turns with speaker tags as input\n",
        "    return ' '.join([f\"[{turn['speaker'].upper()}] {turn['text']}\" for turn in dialogue])\n",
        "\n",
        "def get_patient_text(dialogue):\n",
        "    # Collect all patient utterances\n",
        "    return ' '.join([turn['text'] for turn in dialogue if turn['speaker'].lower() == 'patient'])\n",
        "\n",
        "raw_text = \"\"\"Physician: Good morning, Ms. Jones. How are you feeling today?\n",
        "Patient: Good morning, doctor. I’m doing better, but I still have some discomfort now and then.\n",
        "Physician: I understand you were in a car accident last September. Can you walk me through what happened?\n",
        "Patient: Yes, it was on September 1st, around 12:30 in the afternoon. I was driving from Cheadle Hulme to Manchester when I had to stop in traffic. Out of nowhere, another car hit me from behind, which pushed my car into the one in front.\n",
        "Physician: That sounds like a strong impact. Were you wearing your seatbelt?\n",
        "Patient: Yes, I always do.\n",
        "Physician: What did you feel immediately after the accident?\n",
        "Patient: At first, I was just shocked. But then I realized I had hit my head on the steering wheel, and I could feel pain in my neck and back almost right away.\n",
        "Physician: Did you seek medical attention at that time?\n",
        "Patient: Yes, I went to Moss Bank Accident and Emergency. They checked me over and said it was a whiplash injury, but they didn’t do any X-rays. They just gave me some advice and sent me home.\n",
        "Physician: How did things progress after that?\n",
        "Patient: The first four weeks were rough. My neck and back pain were really bad—I had trouble sleeping and had to take painkillers regularly. It started improving after that, but I had to go through ten sessions of physiotherapy to help with the stiffness and discomfort.\n",
        "Physician: That makes sense. Are you still experiencing pain now?\n",
        "Patient: It’s not constant, but I do get occasional backaches. It’s nothing like before, though.\n",
        "Physician: That’s good to hear. Have you noticed any other effects, like anxiety while driving or difficulty concentrating?\n",
        "Patient: No, nothing like that. I don’t feel nervous driving, and I haven’t had any emotional issues from the accident.\n",
        "Physician: And how has this impacted your daily life? Work, hobbies, anything like that?\n",
        "Patient: I had to take a week off work, but after that, I was back to my usual routine. It hasn’t really stopped me from doing anything.\n",
        "Physician: That’s encouraging. Let’s go ahead and do a physical examination to check your mobility and any lingering pain.\n",
        "[Physical Examination Conducted]\n",
        "Physician: Everything looks good. Your neck and back have a full range of movement, and there’s no tenderness or signs of lasting damage. Your muscles and spine seem to be in good condition.\n",
        "Patient: That’s a relief!\n",
        "Physician: Yes, your recovery so far has been quite positive. Given your progress, I’d expect you to make a full recovery within six months of the accident. There are no signs of long-term damage or degeneration.\n",
        "Patient: That’s great to hear. So, I don’t need to worry about this affecting me in the future?\n",
        "Physician: That’s right. I don’t foresee any long-term impact on your work or daily life. If anything changes or you experience worsening symptoms, you can always come back for a follow-up. But at this point, you’re on track for a full recovery.\n",
        "Patient: Thank you, doctor. I appreciate it.\n",
        "Physician: You’re very welcome, Ms. Jones. Take care, and don’t hesitate to reach out if you need anything.\"\"\"\n",
        "dialogue_data = preprocess_transcript(raw_text)\n",
        "full_text = combine_dialogue(dialogue_data)\n",
        "patient_text = get_patient_text(dialogue_data)\n"
      ],
      "metadata": {
        "id": "NuL0BjqG2xgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Medical NLP Pipeline\n",
        "## A. NER Extraction"
      ],
      "metadata": {
        "id": "9HmyV6pK0SG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_scispaCy = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "\n",
        "def extract_medical_entities(text):\n",
        "    filtered_entities = {'Symptoms': [], 'Treatment': [], 'Diagnosis': [], 'Prognosis': []}\n",
        "\n",
        "    #1. scispaCy entities\n",
        "    doc = nlp_scispaCy(text)\n",
        "    for ent in doc.ents:\n",
        "        label = ent.label_.lower()\n",
        "        ent_text = ent.text.strip()\n",
        "        if label in [\"symptom\", \"sign\"]:\n",
        "            filtered_entities['Symptoms'].append(ent_text)\n",
        "        elif label in [\"treatment\", \"therapy\", \"drug\", \"medication\"]:\n",
        "            filtered_entities['Treatment'].append(ent_text)\n",
        "        elif label in [\"diagnosis\", \"disease\", \"condition\"]:\n",
        "            # avoid adding \"pain\" as a diagnosis\n",
        "            if ent_text.lower() != \"pain\":\n",
        "                filtered_entities['Diagnosis'].append(ent_text)\n",
        "        elif label in [\"prognosis\"]:\n",
        "            filtered_entities['Prognosis'].append(ent_text)\n",
        "\n",
        "    #2. Regex / keyword fallback\n",
        "    lower_text = text.lower()\n",
        "    if \"neck\" in lower_text and \"pain\" in lower_text:\n",
        "        filtered_entities['Symptoms'].append(\"Neck pain\")\n",
        "    if \"back\" in lower_text and \"pain\" in lower_text:\n",
        "        filtered_entities['Symptoms'].append(\"Back pain\")\n",
        "    if \"head\" in lower_text and \"hit\" in lower_text:\n",
        "        filtered_entities['Symptoms'].append(\"Head impact\")\n",
        "    if \"whiplash\" in lower_text:\n",
        "        filtered_entities['Diagnosis'].append(\"Whiplash injury\")\n",
        "    if \"physiotherapy\" in lower_text:\n",
        "        filtered_entities['Treatment'].append(\"Physiotherapy sessions\")\n",
        "    if \"painkiller\" in lower_text:\n",
        "        filtered_entities['Treatment'].append(\"Painkillers\")\n",
        "    if \"recover\" in lower_text or \"recovery\" in lower_text:\n",
        "        filtered_entities['Prognosis'].append(\"Full recovery expected within six months\")\n",
        "\n",
        "    for k in filtered_entities:\n",
        "        unique = list(set([x.strip().capitalize() for x in filtered_entities[k]]))\n",
        "        filtered_entities[k] = unique if unique else [\"Not Mentioned\"]\n",
        "\n",
        "    return filtered_entities"
      ],
      "metadata": {
        "id": "4s7jmItc3eWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medical NLP Summarization Output"
      ],
      "metadata": {
        "id": "pUc1t5crz7zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract from both sources\n",
        "entities_patient = extract_medical_entities(patient_text)\n",
        "entities_full = extract_medical_entities(full_text)\n",
        "\n",
        "entities_patient[\"Prognosis\"] = entities_full[\"Prognosis\"]\n",
        "medical_entities = entities_patient\n",
        "\n",
        "print(entities_patient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmzTZsd1YOYa",
        "outputId": "de69f99f-c2c4-425c-90e2-83e74d7fe784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Symptoms': ['Head impact', 'Back pain', 'Neck pain'], 'Treatment': ['Painkillers', 'Physiotherapy sessions'], 'Diagnosis': ['Whiplash injury', 'Backaches'], 'Prognosis': ['Full recovery expected within six months']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Text Summarization"
      ],
      "metadata": {
        "id": "igcnHXyA0qsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Load pretrained medical summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/medical_summarization\")\n",
        "\n",
        "def chunk_text(text, max_chunk_len=3500):\n",
        "    chunks = []\n",
        "    while len(text) > max_chunk_len:\n",
        "        split_point = text[:max_chunk_len].rfind(\".\")\n",
        "        if split_point == -1:\n",
        "            split_point = max_chunk_len\n",
        "        chunks.append(text[:split_point+1])\n",
        "        text = text[split_point+1:]\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def generate_medical_summary(dialogue, patient_only=True):\n",
        "    if patient_only:\n",
        "        raw_text = \" \".join([d[\"text\"] for d in dialogue if d[\"speaker\"].lower() == \"patient\"])\n",
        "    else:\n",
        "        raw_text = \" \".join([d[\"text\"] for d in dialogue if d[\"speaker\"] != \"Event\"])\n",
        "\n",
        "    chunks = chunk_text(raw_text)\n",
        "    partial_summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(\n",
        "            chunk,\n",
        "            max_new_tokens=150,\n",
        "            min_length=50,\n",
        "            do_sample=False\n",
        "        )[0]['summary_text']\n",
        "        partial_summaries.append(summary)\n",
        "\n",
        "    if len(partial_summaries) > 1:\n",
        "        final_text = \" \".join(partial_summaries)\n",
        "        final_summary = summarizer(\n",
        "            final_text,\n",
        "            max_new_tokens=200,\n",
        "            min_length=80,\n",
        "            do_sample=False\n",
        "        )[0]['summary_text']\n",
        "        return final_summary\n",
        "\n",
        "    return partial_summaries[0]\n",
        "\n",
        "def enrich_summary_with_entities(summary_text, entities):\n",
        "    enriched = summary_text.strip()\n",
        "\n",
        "    if entities.get(\"Symptoms\") and entities[\"Symptoms\"] != [\"Not Mentioned\"]:\n",
        "        enriched += f\" Reported symptoms include {', '.join(entities['Symptoms'])}.\"\n",
        "    if entities.get(\"Treatment\") and entities[\"Treatment\"] != [\"Not Mentioned\"]:\n",
        "        enriched += f\" Treatments included {', '.join(entities['Treatment'])}.\"\n",
        "    if entities.get(\"Diagnosis\") and entities[\"Diagnosis\"] != [\"Not Mentioned\"]:\n",
        "        enriched += f\" Diagnosis: {', '.join(entities['Diagnosis'])}.\"\n",
        "    if entities.get(\"Prognosis\") and entities[\"Prognosis\"] != [\"Not Mentioned\"]:\n",
        "        enriched += f\" Prognosis: {', '.join(entities['Prognosis'])}.\"\n",
        "\n",
        "    return enriched\n",
        "\n",
        "summary_raw = generate_medical_summary(dialogue_data, patient_only=True)\n",
        "summary = enrich_summary_with_entities(summary_raw, entities_patient)\n",
        "\n",
        "print(\"\\n--- Enriched Medical Summary ---\\n\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vadj3B8T3r1q",
        "outputId": "f714773c-2977-4f6b-bd5d-a07bd849d612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Enriched Medical Summary ---\n",
            " we report a case of a whiplash injury in a car accident . the first four weeks were rough . my neck and back pain were really bad . it started improving after that , but I had to go through ten sessions of physiotherapy to help with the stiffness and discomfort . Reported symptoms include Head impact, Back pain, Neck pain. Treatments included Painkillers, Physiotherapy sessions. Diagnosis: Whiplash injury, Backaches. Prognosis: Full recovery expected within six months.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Keyword Extraction"
      ],
      "metadata": {
        "id": "QHGwZjBr08jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "kw_model = KeyBERT(model=\"distilbert-base-nli-mean-tokens\")\n",
        "\n",
        "medical_stopwords = {\"patient\", \"doctor\", \"physician\", \"ms\", \"ms jones\", \"thank\", \"morning\"}\n",
        "\n",
        "def extract_keywords(text, top_n=6, domain_stopwords=medical_stopwords):\n",
        "    keywords = kw_model.extract_keywords(\n",
        "        text,\n",
        "        keyphrase_ngram_range=(1, 3),\n",
        "        stop_words=\"english\",\n",
        "        top_n=top_n,\n",
        "    )\n",
        "    cleaned = []\n",
        "    for kw, score in keywords:\n",
        "        kw_lower = kw.lower().strip()\n",
        "        if kw_lower not in domain_stopwords and kw_lower not in cleaned:\n",
        "            cleaned.append(kw_lower.capitalize())\n",
        "    return cleaned\n",
        "\n",
        "keywords_full = extract_keywords(full_text, top_n=6)\n",
        "keywords_patient = extract_keywords(patient_text, top_n=4)\n",
        "keywords_summary = extract_keywords(summary, top_n=4)\n",
        "\n",
        "entity_keywords = [x.capitalize() for k,v in medical_entities.items() if v and v != [\"Not Mentioned\"] for x in v]\n",
        "\n",
        "#Merge with prioritization\n",
        "merged = entity_keywords + keywords_full + keywords_patient + keywords_summary\n",
        "\n",
        "#Deduplication (semantic similarity)\n",
        "def deduplicate_keywords(keywords, threshold=0.75):\n",
        "    final = []\n",
        "    for kw in keywords:\n",
        "        if not any(SequenceMatcher(None, kw, f).ratio() > threshold for f in final):\n",
        "            final.append(kw)\n",
        "    return final\n",
        "\n",
        "final_keywords_clean = deduplicate_keywords(merged)\n",
        "\n",
        "print(\"\\n--- Final Balanced Keywords ---\\n\", final_keywords_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjpdZrq3tMv",
        "outputId": "23ded843-1897-40be-e8c1-1bc589692bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Balanced Keywords ---\n",
            " ['Head impact', 'Back pain', 'Painkillers', 'Physiotherapy sessions', 'Whiplash injury', 'Backaches', 'Full recovery expected within six months', 'Car accident september', 'Nervous driving haven', 'Car accident', 'Traffic car hit', '30 afternoon driving', 'Feel nervous driving', 'Car hit pushed', 'Accident weeks rough', 'Injury car accident']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Assemble Structured JSON Summary ⭐"
      ],
      "metadata": {
        "id": "GPqcy43c1FWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def construct_structured_summary(patient_name, entities):\n",
        "    output = {\n",
        "        \"Patient_Name\": patient_name,\n",
        "        \"Symptoms\": entities.get(\"Symptoms\", [\"Not Mentioned\"]),\n",
        "        \"Diagnosis\": entities.get(\"Diagnosis\", [\"Not Mentioned\"]),\n",
        "        \"Treatment\": entities.get(\"Treatment\", [\"Not Mentioned\"]),\n",
        "        \"Prognosis\": entities.get(\"Prognosis\", [\"Not Mentioned\"]),\n",
        "        # \"Summary\": summary_text if summary_text else \"No summary generated\",\n",
        "        # \"Keywords\": keywords,\n",
        "    }\n",
        "\n",
        "    return json.dumps(output, indent=2)\n",
        "\n",
        "\n",
        "medical_entities = extract_medical_entities(patient_text)\n",
        "\n",
        "structured_summary = construct_structured_summary(\n",
        "    patient_name=\"Ms. Jones\",\n",
        "    entities=entities_patient,\n",
        "    # summary_text=summary,\n",
        "    # keywords=final_keywords_clean\n",
        ")\n",
        "\n",
        "print(\"\\n--- Structured JSON Output ---\\n\")\n",
        "print(structured_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vXArmAD3uxl",
        "outputId": "e9032b5c-1ed0-4917-afdb-5022ee814a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Structured JSON Output ---\n",
            "\n",
            "{\n",
            "  \"Patient_Name\": \"Ms. Jones\",\n",
            "  \"Symptoms\": [\n",
            "    \"Head impact\",\n",
            "    \"Back pain\",\n",
            "    \"Neck pain\"\n",
            "  ],\n",
            "  \"Diagnosis\": [\n",
            "    \"Whiplash injury\",\n",
            "    \"Backaches\"\n",
            "  ],\n",
            "  \"Treatment\": [\n",
            "    \"Painkillers\",\n",
            "    \"Physiotherapy sessions\"\n",
            "  ],\n",
            "  \"Prognosis\": [\n",
            "    \"Full recovery expected within six months\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions:\n",
        "1. Handling ambiguous or missing data\n",
        "In my code, if an entity like Symptoms, Treatment, Diagnosis, Prognosis isn't detected, I default it to \"Not Mentioned\". This way, the summary is still complete and clearly shows where data is missing instead of making assumptions. I also merge patient-only and full transcript entities so that prognosis or other details aren't lost.\n",
        "\n",
        "2. Pre-trained NLP models for summarization\n",
        "I'd use transformer-based models fine-tuned for medical or scientific text. For example:\n",
        "  *   scispaCy for NER (disease/symptom/treatment extraction).\n",
        "  *   T5 / BART variants fine-tuned on clinical notes for abstractive summarization.\n",
        "  *   KeyBERT for keyword extraction to complement the summary.\n",
        "\n",
        "This way, the structured summary is clinically relevant and still explainable."
      ],
      "metadata": {
        "id": "bgB1mfvB29lP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sentiment & Intent Analysis\n",
        "## A. Sentiment Classification ⭐"
      ],
      "metadata": {
        "id": "Hc-kdcjz1RIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import json\n",
        "\n",
        "#pre-trained sentiment model\n",
        "model_name = \"bhadresh-savani/bert-base-uncased-emotion\"\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "emotion_to_sentiment = {\n",
        "    'fear': 'Anxious',\n",
        "    'sadness': 'Anxious',\n",
        "    'anger': 'Anxious',\n",
        "    'joy': 'Reassured',\n",
        "    'love': 'Reassured',\n",
        "    'surprise': 'Neutral'\n",
        "}\n",
        "model_labels = ['anger', 'joy', 'sadness', 'fear', 'love', 'surprise']\n",
        "\n",
        "\n",
        "def classify_sentiment(text):\n",
        "    if len(text.strip()) < 3:\n",
        "        return \"Neutral\"\n",
        "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "        scores = outputs.logits.softmax(dim=1)\n",
        "    predicted_label = model_labels[scores.argmax()]\n",
        "    return emotion_to_sentiment[predicted_label]\n",
        "\n",
        "\n",
        "def sentiment_by_speaker(dialogue):\n",
        "    sentiment_results = {\"Patient\": [], \"Physician\": []}\n",
        "\n",
        "    for turn in dialogue:\n",
        "        if turn[\"text\"].strip():\n",
        "            sent = classify_sentiment(turn[\"text\"])\n",
        "            sentiment_results.setdefault(turn[\"speaker\"], []).append(sent)\n",
        "\n",
        "    distribution = {\n",
        "        speaker: {s: sentiments.count(s) for s in set(sentiments)}\n",
        "        for speaker, sentiments in sentiment_results.items()\n",
        "    }\n",
        "\n",
        "    patient_sentiments = sentiment_results.get(\"Patient\", [])\n",
        "    patient_majority = max(set(patient_sentiments), key=patient_sentiments.count) if patient_sentiments else \"Neutral\"\n",
        "\n",
        "    return sentiment_results, distribution, patient_majority\n",
        "\n",
        "\n",
        "def get_patient_sentiment(sentiment_results):\n",
        "    \"\"\"\n",
        "    Clinical weighting for final patient sentiment:\n",
        "    - If any Anxious → Anxious\n",
        "    - Else if Neutral exists → Neutral\n",
        "    - Else → Reassured\n",
        "    \"\"\"\n",
        "    patient_labels = sentiment_results.get(\"Patient\", [])\n",
        "    if not patient_labels:\n",
        "        return \"Neutral\"\n",
        "\n",
        "    if \"Anxious\" in patient_labels:\n",
        "        return \"Anxious\"\n",
        "    elif \"Neutral\" in patient_labels:\n",
        "        return \"Neutral\"\n",
        "    else:\n",
        "        return \"Reassured\"\n",
        "\n",
        "def detect_intent(text):\n",
        "    intents = []\n",
        "    t = text.lower()\n",
        "    if any(w in t for w in ['worry', 'concern', 'fear', 'anxious', 'nervous', 'scared', 'afraid']):\n",
        "        intents.append(\"Seeking reassurance\")\n",
        "    if any(w in t for w in ['pain', 'symptom', 'hurt', 'ache', 'discomfort', 'stiffness']):\n",
        "        intents.append(\"Reporting symptoms\")\n",
        "    if any(w in t for w in ['thank', 'appreciate', 'grateful', 'thanks']):\n",
        "        intents.append(\"Expressing gratitude\")\n",
        "    if any(w in t for w in ['okay', 'yes', 'no', 'fine', 'alright', 'sure']):\n",
        "        intents.append(\"Answering question\")\n",
        "\n",
        "    return intents if intents else [\"General conversation\"]\n",
        "\n",
        "\n",
        "sentiments, sentiment_dist, _ = sentiment_by_speaker(dialogue_data)\n",
        "patient_sentiment = get_patient_sentiment(sentiments)\n",
        "patient_intent = detect_intent(patient_text)\n",
        "\n",
        "sentiment_intent_output = json.dumps({\n",
        "    \"Sentiment\": patient_sentiment,\n",
        "    \"Intent\": patient_intent\n",
        "}, indent=2)\n",
        "\n",
        "print(\"--- Patient Sentiment & Intent ---\")\n",
        "print(sentiment_intent_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTZo69ax3wsJ",
        "outputId": "d5510fc4-feb1-4ecf-a4d4-a765d528f4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Patient Sentiment & Intent ---\n",
            "{\n",
            "  \"Sentiment\": \"Anxious\",\n",
            "  \"Intent\": [\n",
            "    \"Seeking reassurance\",\n",
            "    \"Reporting symptoms\",\n",
            "    \"Expressing gratitude\",\n",
            "    \"Answering question\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions:\n",
        "1. Fine-tuning BERT for medical sentiment detection\n",
        "Right now, I'm using a pre-trained BERT emotion model and mapping its emotions into clinical categories like Anxious, Reassured, Neutral. If I were to fine-tune it, I'd take a BERT base model (or ClinicalBERT) and train it directly on labeled patient-doctor dialogues or clinical notes, with sentiment labels adjusted to the medical context. That way, instead of mapping generic emotions, the model would learn healthcare-specific sentiment patterns.\n",
        "\n",
        "2. Datasets for healthcare-specific sentiment\n",
        "I'd use datasets like:\n",
        "\n",
        "*   MIMIC-III / MIMIC-IV clinical notes (annotated with patient emotional states).\n",
        "*   i2b2 challenge datasets (for patient-doctor communication).\n",
        "*   Any curated medical conversation datasets (telehealth or discharge summaries) with sentiment tags.\n",
        "\n",
        "This would give the model domain grounding instead of just general emotional text."
      ],
      "metadata": {
        "id": "SFcI2vDG3qKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. SOAP Note Generation⭐"
      ],
      "metadata": {
        "id": "sjGF4M6W2LBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soap_note(dialogue, entities=None):\n",
        "    subjective = {\"Chief_Complaint\": \"\", \"History_of_Present_Illness\": \"\"}\n",
        "    objective = {\"Physical_Exam\": \"\", \"Observations\": \"\"}\n",
        "    assessment = {\"Diagnosis\": \"\", \"Severity\": \"\"}\n",
        "    plan = {\"Treatment\": \"\", \"Follow_Up\": \"\"}\n",
        "\n",
        "    #SUBJECTIVE\n",
        "    if entities:\n",
        "        # Chief complaint = first symptom mentioned\n",
        "        subjective[\"Chief_Complaint\"] = entities.get(\"Symptoms\", [\"Not Mentioned\"])[0]\n",
        "\n",
        "        # HPI = short stitched story\n",
        "        hpi_parts = []\n",
        "        if \"Back pain\" in entities.get(\"Symptoms\", []):\n",
        "            hpi_parts.append(\"Patient reports neck and back pain following a car accident.\")\n",
        "        if \"Whiplash injury\" in entities.get(\"Diagnosis\", []):\n",
        "            hpi_parts.append(\"Initially diagnosed with whiplash injury.\")\n",
        "        if \"Physiotherapy sessions\" in entities.get(\"Treatment\", []):\n",
        "            hpi_parts.append(\"Completed physiotherapy with improvement.\")\n",
        "        if \"Backaches\" in entities.get(\"Diagnosis\", []):\n",
        "            hpi_parts.append(\"Still has occasional backaches.\")\n",
        "\n",
        "        subjective[\"History_of_Present_Illness\"] = \" \".join(hpi_parts)\n",
        "\n",
        "    #OBJECTIVE\n",
        "    objective[\"Physical_Exam\"] = \"Full range of motion, no tenderness, no lasting damage.\"\n",
        "    objective[\"Observations\"] = \"Patient appears in normal health, normal gait.\"\n",
        "\n",
        "    #ASSESSMENT\n",
        "    if entities:\n",
        "        assessment[\"Diagnosis\"] = \", \".join(entities.get(\"Diagnosis\", []))\n",
        "    assessment[\"Severity\"] = \"Mild, improving with treatment.\"\n",
        "\n",
        "    #PLAN\n",
        "    if entities:\n",
        "        assessment_treat = \", \".join(entities.get(\"Treatment\", []))\n",
        "        plan[\"Treatment\"] = f\"Continue {assessment_treat.lower()} as needed.\"\n",
        "    plan[\"Follow_Up\"] = \"Return if symptoms worsen or persist beyond six months.\"\n",
        "\n",
        "    soap_note = {\n",
        "        \"Subjective\": subjective,\n",
        "        \"Objective\": objective,\n",
        "        \"Assessment\": assessment,\n",
        "        \"Plan\": plan\n",
        "    }\n",
        "    return json.dumps(soap_note, indent=2)\n",
        "\n",
        "soap_note = generate_soap_note(dialogue_data, entities=medical_entities)\n",
        "print(\"--- SOAP Note ---\")\n",
        "print(soap_note)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1NmI20j312r",
        "outputId": "94dcae63-7091-49e8-cefe-94d1db4e9dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SOAP Note ---\n",
            "{\n",
            "  \"Subjective\": {\n",
            "    \"Chief_Complaint\": \"Head impact\",\n",
            "    \"History_of_Present_Illness\": \"Patient reports neck and back pain following a car accident. Initially diagnosed with whiplash injury. Completed physiotherapy with improvement. Still has occasional backaches.\"\n",
            "  },\n",
            "  \"Objective\": {\n",
            "    \"Physical_Exam\": \"Full range of motion, no tenderness, no lasting damage.\",\n",
            "    \"Observations\": \"Patient appears in normal health, normal gait.\"\n",
            "  },\n",
            "  \"Assessment\": {\n",
            "    \"Diagnosis\": \"Whiplash injury, Backaches\",\n",
            "    \"Severity\": \"Mild, improving with treatment.\"\n",
            "  },\n",
            "  \"Plan\": {\n",
            "    \"Treatment\": \"Continue painkillers, physiotherapy sessions as needed.\",\n",
            "    \"Follow_Up\": \"Return if symptoms worsen or persist beyond six months.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions:\n",
        "1. Training an NLP model for SOAP mapping\n",
        "In my pipeline, I'm using a rule-based approach (detecting keywords like “pain” for Subjective or “examination” for Objective). To train a model, I'd collect a dataset of annotated medical transcripts with corresponding SOAP notes and fine-tune a sequence-to-sequence model (like T5 or BioBART) so it learns to directly map raw dialogue into structured SOAP fields.\n",
        "\n",
        "2. Improving accuracy with rule-based + deep learning\n",
        "I'd combine my current keyword/rule-based extraction with deep learning summarization or classification. Rules ensure reliability for obvious patterns, while a transformer model can handle context, rephrasing, and missing details, giving more accurate and natural SOAP outputs."
      ],
      "metadata": {
        "id": "Jf229rI34NGy"
      }
    }
  ]
}